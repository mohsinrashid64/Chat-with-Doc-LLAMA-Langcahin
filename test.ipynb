{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 2048  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"models/llama-2-7b-chat.gguf.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt | llm\n",
    "question = \"what is cricket\"\n",
    "llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = LlamaCppEmbeddings(model_path='models/llama-2-7b-chat.gguf.q4_0.bin',verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"This is a test document.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_result = [float(item) for sublist in query_result for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flattened_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(self, text: str):\n",
    "    \"\"\"Embed a query using the Llama model.\n",
    "\n",
    "    Args:\n",
    "        text: The text to embed.\n",
    "\n",
    "    Returns:\n",
    "        Embeddings for the text.\n",
    "    \"\"\"\n",
    "    embedding = self.client.embed(text)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain with Pincecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = LlamaCppEmbeddings(model_path='models/llama-2-7b-chat.gguf.q4_0.bin',verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_v = PineconeVectorStore(index_name='powerfule',pinecone_api_key=os.environ.get('PINECONE_API_KEY'),embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"test.txt\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=20,separators='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create_documents in module langchain_text_splitters.base:\n",
      "\n",
      "create_documents(texts: 'List[str]', metadatas: 'Optional[List[dict]]' = None) -> 'List[Document]' method of langchain_text_splitters.character.RecursiveCharacterTextSplitter instance\n",
      "    Create documents from a list of texts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(splitter.create_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = splitter.create_documents(texts=['the powerful yello panther leaps about the lethargic dog species'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='the powerful yello panther leaps about the'),\n",
       " Document(page_content='leaps about the lethargic dog species')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedings LEN 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bf5bd270-72a3-454c-a193-717d3c3bfccf']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_v.add_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedings LEN 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7f4fb017-e8f2-4f81-bb2a-e30d0c88e317',\n",
       " '34b19f17-6939-4550-8043-5e4992c66443',\n",
       " '400c09c4-d1ef-4041-af3c-c30bc3cdee50',\n",
       " 'a6aaac90-1bbb-40be-a2a4-cc8cac19735a',\n",
       " '1a7c972c-c53d-4e6d-9eca-d92248e62396',\n",
       " '8f269294-6d84-48a5-af5b-43a260f68f47',\n",
       " '4c924870-293d-48b0-a61d-b298209c26ec',\n",
       " '2c6475a0-187a-4d6a-8fd6-ad5a1d37a529',\n",
       " '8460776d-6195-4e93-a563-0be57475a030',\n",
       " '9967035a-386b-4869-9012-8705383dcd92',\n",
       " 'bcf98835-7c74-4998-ad71-86cb7f5ddf8a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_v.add_texts('apple power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(PineconeVectorStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.environ.get('PINECONE_API_KEY')) # Setting Pine Cone API Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 4096,\n",
       " 'host': 'powerfule-9g87gt2.svc.aped-4627-b74a.pinecone.io',\n",
       " 'metric': 'cosine',\n",
       " 'name': 'powerfule',\n",
       " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       " 'status': {'ready': True, 'state': 'Ready'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.describe_index('powerfule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index('powerfule', pool_threads = 32)\n",
    "doc_ids = sum([ids for ids in index.list()], [])\n",
    "index_data  = index.fetch(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method list_indexes in module pinecone.control.pinecone:\n",
      "\n",
      "list_indexes() -> pinecone.models.index_list.IndexList method of pinecone.control.pinecone.Pinecone instance\n",
      "    Lists all indexes.\n",
      "    \n",
      "    The results include a description of all indexes in your project, including the \n",
      "    index name, dimension, metric, status, and spec.\n",
      "    \n",
      "    :return: Returns an `IndexList` object, which is iterable and contains a \n",
      "        list of `IndexDescription` objects. It also has a convenience method `names()`\n",
      "        which returns a list of index names.\n",
      "    \n",
      "    ```python\n",
      "    from pinecone import Pinecone\n",
      "    \n",
      "    client = Pinecone()\n",
      "    \n",
      "    index_name = \"my_index\"\n",
      "    if index_name not in client.list_indexes().names():\n",
      "        print(\"Index does not exist, creating...\")\n",
      "        client.create_index(\n",
      "            name=index_name,\n",
      "            dimension=768,\n",
      "            metric=\"cosine\",\n",
      "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
      "        )\n",
      "    ```\n",
      "    \n",
      "    You can also use the `list_indexes()` method to iterate over all indexes in your project\n",
      "    and get other information besides just names.\n",
      "    \n",
      "    ```python\n",
      "    from pinecone import Pinecone\n",
      "    \n",
      "    client = Pinecone()\n",
      "    \n",
      "    for index in client.list_indexes():\n",
      "        print(index.name)\n",
      "        print(index.dimension)\n",
      "        print(index.metric)\n",
      "        print(index.status)\n",
      "        print(index.host)\n",
      "        print(index.spec)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
